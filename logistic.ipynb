{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import log_loss\n",
    "from softmax import softmax, log_softmax\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from numpy.linalg import inv, eigh\n",
    "from random import sample\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X, y = datasets.htwt()\n",
    "X, y = datasets.iris()\n",
    "Y = datasets.one_hot(y)\n",
    "N, D = X.shape\n",
    "N, C = Y.shape\n",
    "Xaug = np.hstack([np.ones((N, 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = T.dvector()\n",
    "decode = lambda params: (params[1:].reshape((D, 1)), params[0])\n",
    "mu = lambda w, b: T.nnet.sigmoid(T.dot(X, w) + b)\n",
    "loss = T.sum(T.nnet.binary_crossentropy(mu(*decode(z)), y.reshape((N, 1))))\n",
    "grad = theano.function([z], T.grad(loss, z))\n",
    "hess = theano.function([z], T.hessian(loss, z))\n",
    "loss = theano.function([z], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = lambda w: expit(Xaug.dot(w))\n",
    "loss = lambda w: log_loss(y, mu(w), normalize = False)\n",
    "grad = lambda w: Xaug.T.dot(mu(w) - y)\n",
    "S = lambda mu: np.diag(mu * (1 - mu))\n",
    "hess = lambda w: Xaug.T.dot(S(mu(w))).dot(Xaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.0] * (D + 1))\n",
    "w = minimize(loss, w).x\n",
    "w = minimize(loss, w, method = 'Newton-CG', jac = grad, hess = hess, tol = 1e-6).x\n",
    "yhat = expit(Xaug.dot(w)) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode = lambda params: params.reshape((D + 1, C))\n",
    "z = T.dvector()\n",
    "mu = lambda W: T.nnet.softmax(T.dot(Xaug, W))\n",
    "loss = T.sum(T.nnet.categorical_crossentropy(mu(decode(z)), Y))\n",
    "grad = theano.function([z], T.grad(loss, z))\n",
    "hess = theano.function([z], T.hessian(loss, z))    \n",
    "loss = theano.function([z], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode = lambda params: np.array(params).reshape((D + 1, C))\n",
    "\n",
    "mu = lambda W: softmax(Xaug.dot(W))\n",
    "loss = lambda params: log_loss(y, mu(decode(params)), normalize = False)\n",
    "\n",
    "def grad(params):\n",
    "    Z = mu(decode(params)) - Y\n",
    "    return sum([np.kron(Xaug[i], z) for i, z in enumerate(Z)])\n",
    "\n",
    "def hess(params):\n",
    "    Mu = mu(decode(params))\n",
    "    o = lambda x: np.outer(x, x)\n",
    "    return sum([np.kron(o(Xaug[i]), np.diag(z) - o(z)) for i, z in enumerate(Mu)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.114\n"
     ]
    }
   ],
   "source": [
    "params = [0] * (D + 1) * C\n",
    "params = minimize(loss, params, method = 'Newton-CG', jac = grad, hess = hess, tol = 1e-6).x\n",
    "W = params.reshape((D + 1, C))\n",
    "yhat = np.argmax(softmax(Xaug.dot(W)), axis = 1)\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bayesian logistic regression\n",
    "mvn = stats.multivariate_normal\n",
    "log_prior = lambda w: mvn.logpdf(w[1:], np.zeros(D), 100 * np.eye(D))\n",
    "mu = lambda w: expit(Xaug.dot(w))\n",
    "nll = lambda w: log_loss(y, mu(w), normalize = False)\n",
    "E = lambda w: nll(w) - log_prior(w)\n",
    "mN = minimize(E, np.zeros(D + 1)).x\n",
    "S = lambda mu: np.diag(mu * (1 - mu))\n",
    "H = Xaug.T.dot(S(mu(mN))).dot(Xaug) + 1.0 / 200\n",
    "VN = inv(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119\n"
     ]
    }
   ],
   "source": [
    "# monte carlo approximation\n",
    "posterior = mvn(mN, VN)\n",
    "yhat = np.mean([mu(w) for w in posterior.rvs(1000)], axis = 0) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.110\n"
     ]
    }
   ],
   "source": [
    "# probit approximation\n",
    "yhat = (1 + np.pi * Xaug.dot(VN).dot(Xaug.T) / 8) ** -0.5 * Xaug.dot(mN) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.219\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent\n",
    "w = np.array([0.0] * (D + 1))\n",
    "s = w.copy()\n",
    "tau0 = 1e-6\n",
    "eta = 1e-2\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "\n",
    "for _ in xrange(epochs):\n",
    "    i = sample(range(N), batch_size)\n",
    "    mu = expit(Xaug[i].dot(w))\n",
    "    g = Xaug[i].T.dot(mu - y[i])\n",
    "    s += g ** 2\n",
    "    w -= eta * g / (tau0 + np.sqrt(s)) # adagrad\n",
    "\n",
    "yhat = expit(Xaug.dot(w)) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020\n"
     ]
    }
   ],
   "source": [
    "# fisher's linear discriminant algorithm\n",
    "mus = []\n",
    "\n",
    "L = 2\n",
    "aaT = lambda a: np.outer(a, a)\n",
    "aTa = lambda a: np.array(a).T.dot(a)\n",
    "mu = np.mean(X)\n",
    "SW = np.zeros((D, D))\n",
    "SB = np.zeros((D, D))\n",
    "\n",
    "for c in xrange(C):\n",
    "    Xc = X[y == c]\n",
    "    muc = np.mean(Xc, axis = 0) * 1.0\n",
    "    mus.append(muc)\n",
    "    SW += sum([aaT(xc - muc) for xc in Xc])\n",
    "    SB += len(Xc) * aaT(muc - mu)\n",
    "\n",
    "if C == 2:\n",
    "    W = inv(SW).dot(mus[1] - mus[0])\n",
    "else:\n",
    "    SW_inv_root = sqrtm(inv(SW))\n",
    "    _, v = eigh(SW_inv_root.dot(SB).dot(SW_inv_root))\n",
    "    U = v[:, : -L - 1 : -1]\n",
    "    W = SW_inv_root.dot(U)\n",
    "    \n",
    "Z = X.dot(W)\n",
    "d = np.array([[aTa(z - mc) for z in Z] for mc in np.dot(mus, W)]).T\n",
    "yhat = np.argmin(d, axis = 1) \n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
