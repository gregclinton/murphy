{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import log_loss\n",
    "from softmax import softmax, log_softmax\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from numpy.linalg import inv, eigh\n",
    "from random import sample\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(datasets)\n",
    "trX, teX, trY, teY = datasets.spam()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(trX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00, 0.079, 0.076\n",
      "0.10, 0.078, 0.076\n",
      "0.20, 0.079, 0.077\n",
      "0.30, 0.078, 0.076\n",
      "0.40, 0.078, 0.076\n",
      "0.50, 0.078, 0.076\n",
      "0.60, 0.078, 0.076\n",
      "0.70, 0.078, 0.076\n",
      "0.80, 0.077, 0.076\n",
      "0.90, 0.077, 0.076\n",
      "1.00, 0.077, 0.075\n",
      "1.10, 0.075, 0.074\n",
      "1.20, 0.077, 0.075\n",
      "1.30, 0.085, 0.077\n",
      "1.40, 0.085, 0.077\n",
      "1.50, 0.078, 0.075\n",
      "1.60, 0.078, 0.075\n",
      "1.70, 0.080, 0.076\n",
      "1.80, 0.080, 0.075\n",
      "1.90, 0.080, 0.075\n",
      "2.00, 0.080, 0.075\n"
     ]
    }
   ],
   "source": [
    "D = trX.shape[1]\n",
    "aug = lambda X: np.hstack([np.ones((len(X), 1)), X])\n",
    "trXaug = aug(trX)\n",
    "teXaug = aug(teX)\n",
    "\n",
    "# X = scaler.transform(trX)\n",
    "# X = np.array([np.log(x + 0.1) for x in trX])\n",
    "#X = np.array([x > 0 for x in trX]) * 1\n",
    "\n",
    "def fit(X, y, l2):\n",
    "    aTa = lambda a: np.dot(a, a)\n",
    "    Hl2 = 2 * l2 * np.eye(D + 1)\n",
    "    Hl2[0, 0] = 0\n",
    "    gl2 = lambda w: 2 * l2 * np.hstack(([0], w[1:]))\n",
    "    mu = lambda w: expit(X.dot(w))\n",
    "    loss = lambda w: log_loss(y, mu(w), normalize = False)\n",
    "    obj = lambda w: loss(w) + l2 * aTa(w[1:]) \n",
    "    grad = lambda w: X.T.dot(mu(w) - y) + gl2(w)\n",
    "    S = lambda mu: np.diag(mu * (1 - mu))\n",
    "    hess = lambda w: X.T.dot(S(mu(w))).dot(X) + Hl2\n",
    "    w = np.array([0.0] * (D + 1))\n",
    "    return minimize(obj, w, method = 'Newton-CG', jac = grad, hess = hess, tol = 1e-6).x\n",
    "\n",
    "def trial(trX, trY, teX, teY, l2):\n",
    "    w = fit(trX, trY, l2)\n",
    "    err = lambda X, y: np.mean((expit(X.dot(w)) > 0.5) * 1 != y)\n",
    "    return l2, err(trX, trY), err(teX, teY)  \n",
    " \n",
    "for l2 in np.linspace(0, 2, 21):\n",
    "    print '%0.2f, %0.3f, %0.3f' % trial(trXaug, trY, teXaug, teY, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X, y = datasets.htwt()\n",
    "X, y = datasets.iris()\n",
    "Y = datasets.one_hot(y)\n",
    "N, D = X.shape\n",
    "N, C = Y.shape\n",
    "Xaug = np.hstack([np.ones((N, 1)), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = T.dvector()\n",
    "decode = lambda params: (params[1:].reshape((D, 1)), params[0])\n",
    "mu = lambda w, b: T.nnet.sigmoid(T.dot(X, w) + b)\n",
    "loss = T.sum(T.nnet.binary_crossentropy(mu(*decode(z)), y.reshape((N, 1))))\n",
    "grad = theano.function([z], T.grad(loss, z))\n",
    "hess = theano.function([z], T.hessian(loss, z))\n",
    "loss = theano.function([z], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = lambda w: expit(Xaug.dot(w))\n",
    "loss = lambda w: log_loss(y, mu(w), normalize = False)\n",
    "grad = lambda w: Xaug.T.dot(mu(w) - y)\n",
    "S = lambda mu: np.diag(mu * (1 - mu))\n",
    "hess = lambda w: Xaug.T.dot(S(mu(w))).dot(Xaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.0] * (D + 1))\n",
    "w = minimize(loss, w).x\n",
    "w = minimize(loss, w, method = 'Newton-CG', jac = grad, hess = hess, tol = 1e-6).x\n",
    "yhat = expit(Xaug.dot(w)) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode = lambda params: params.reshape((D + 1, C))\n",
    "z = T.dvector()\n",
    "mu = lambda W: T.nnet.softmax(T.dot(Xaug, W))\n",
    "loss = T.sum(T.nnet.categorical_crossentropy(mu(decode(z)), Y))\n",
    "grad = theano.function([z], T.grad(loss, z))\n",
    "hess = theano.function([z], T.hessian(loss, z))    \n",
    "loss = theano.function([z], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode = lambda params: np.array(params).reshape((D + 1, C))\n",
    "\n",
    "mu = lambda W: softmax(Xaug.dot(W))\n",
    "loss = lambda params: log_loss(y, mu(decode(params)), normalize = False)\n",
    "\n",
    "def grad(params):\n",
    "    Z = mu(decode(params)) - Y\n",
    "    return sum([np.kron(Xaug[i], z) for i, z in enumerate(Z)])\n",
    "\n",
    "def hess(params):\n",
    "    Mu = mu(decode(params))\n",
    "    o = lambda x: np.outer(x, x)\n",
    "    return sum([np.kron(o(Xaug[i]), np.diag(z) - o(z)) for i, z in enumerate(Mu)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.114\n"
     ]
    }
   ],
   "source": [
    "params = [0] * (D + 1) * C\n",
    "params = minimize(loss, params, method = 'Newton-CG', jac = grad, hess = hess, tol = 1e-6).x\n",
    "W = params.reshape((D + 1, C))\n",
    "yhat = np.argmax(softmax(Xaug.dot(W)), axis = 1)\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bayesian logistic regression\n",
    "mvn = stats.multivariate_normal\n",
    "log_prior = lambda w: mvn.logpdf(w[1:], np.zeros(D), 100 * np.eye(D))\n",
    "mu = lambda w: expit(Xaug.dot(w))\n",
    "nll = lambda w: log_loss(y, mu(w), normalize = False)\n",
    "E = lambda w: nll(w) - log_prior(w)\n",
    "mN = minimize(E, np.zeros(D + 1)).x\n",
    "S = lambda mu: np.diag(mu * (1 - mu))\n",
    "H = Xaug.T.dot(S(mu(mN))).dot(Xaug) + 1.0 / 200\n",
    "VN = inv(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119\n"
     ]
    }
   ],
   "source": [
    "# monte carlo approximation\n",
    "posterior = mvn(mN, VN)\n",
    "yhat = np.mean([mu(w) for w in posterior.rvs(1000)], axis = 0) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.110\n"
     ]
    }
   ],
   "source": [
    "# probit approximation\n",
    "yhat = (1 + np.pi * Xaug.dot(VN).dot(Xaug.T) / 8) ** -0.5 * Xaug.dot(mN) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.219\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent\n",
    "w = np.array([0.0] * (D + 1))\n",
    "s = w.copy()\n",
    "tau0 = 1e-6\n",
    "eta = 1e-2\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "\n",
    "for _ in xrange(epochs):\n",
    "    i = sample(range(N), batch_size)\n",
    "    mu = expit(Xaug[i].dot(w))\n",
    "    g = Xaug[i].T.dot(mu - y[i])\n",
    "    s += g ** 2\n",
    "    w -= eta * g / (tau0 + np.sqrt(s)) # adagrad\n",
    "\n",
    "yhat = expit(Xaug.dot(w)) > 0.5\n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020\n"
     ]
    }
   ],
   "source": [
    "# fisher's linear discriminant algorithm\n",
    "mus = []\n",
    "\n",
    "L = 2\n",
    "aaT = lambda a: np.outer(a, a)\n",
    "aTa = lambda a: np.array(a).T.dot(a)\n",
    "mu = np.mean(X)\n",
    "SW = np.zeros((D, D))\n",
    "SB = np.zeros((D, D))\n",
    "\n",
    "for c in xrange(C):\n",
    "    Xc = X[y == c]\n",
    "    muc = np.mean(Xc, axis = 0) * 1.0\n",
    "    mus.append(muc)\n",
    "    SW += sum([aaT(xc - muc) for xc in Xc])\n",
    "    SB += len(Xc) * aaT(muc - mu)\n",
    "\n",
    "if C == 2:\n",
    "    W = inv(SW).dot(mus[1] - mus[0])\n",
    "else:\n",
    "    SW_inv_root = sqrtm(inv(SW))\n",
    "    _, v = eigh(SW_inv_root.dot(SB).dot(SW_inv_root))\n",
    "    U = v[:, : -L - 1 : -1]\n",
    "    W = SW_inv_root.dot(U)\n",
    "    \n",
    "Z = X.dot(W)\n",
    "d = np.array([[aTa(z - mc) for z in Z] for mc in np.dot(mus, W)]).T\n",
    "yhat = np.argmin(d, axis = 1) \n",
    "print '%0.3f' % np.mean(yhat != y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
